{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('winequalityred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [i.replace(\"\\\"\",\"\").replace('\\'','') for i in df.columns[0].split(';')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df.iloc[:,0].str.split(';').tolist(),columns = col_names)\n",
    "df = df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('quality', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=.2, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression equations and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " y =  \\sum_{j=1}^{p} X_{j}*B_{j} \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "RSS =  \\sum_{i=1}^{N} (y_{i} - B_{o} - \\sum_{j=1}^{p} x_{ij}*B_{j})^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_predict(X, beta):\n",
    "    return X.apply(lambda z: np.dot(z, beta), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(beta, X, y):\n",
    "    return ((y - lm_predict(X, beta))**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(RSS, x0= np.random.normal(0,1,X_train.shape[1]), args = (X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.19185222e+00,  8.74711336e-01,  3.00696788e-01,  1.31235567e-02,\n",
       "        8.44069623e-03,  3.02578751e-03, -2.87979385e-03, -1.79425350e-01,\n",
       "       -4.27624486e-01, -1.17657060e+00, -1.93704449e+00])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat[(-beta_hat).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['density', 'sulphates', 'alcohol', 'residual sugar', 'fixed acidity',\n",
       "       'free sulfur dioxide', 'total sulfur dioxide', 'citric acid', 'pH',\n",
       "       'volatile acidity', 'chlorides'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###largest to smallest\n",
    "X.columns[(-beta_hat).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.46779237023139"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X['total sulfur dioxide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967466791744833"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X['density'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features that are positively correlated to good quality are density, sulphates, alcohol, residual sugar, fixed acidity, free sulfur dioxide. \n",
    "\n",
    "Features that are negatively correlated to good quality are citric acid, pH, volaitile acitidy and chlorides. \n",
    "\n",
    "Scale of the features can have a large impact to the size of the coefficients. For example, density is averaged at .9967. So if we were to increase it by just one increment that would be double the average. On the other hand, total sulfur dioxide's average is at 46.46 so increment of one is not a proportionally large change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSS in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.5359871242772"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS(beta_hat, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122.42627202457935"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS(beta_hat, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9787293672561646, 0.00011085698497481644)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro((y_test-lm_predict(X_test, beta_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model fits relatively well in test data than in the train data. Our RSS is much smaller in the test data at 122.42 as opposed to 545, which suggests that the model generalizes well. However, when we run the shaprio wilks test on the residuals, we see that we reject the null hypothesis that the residuals are normally distributed, which is a strong assumption of linear models. Therefore, there is a reason to believe that linear regression does not fit very well in this data and some of the assumptions are violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Different Initial $B_o$ Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_zeros = minimize(RSS, x0= np.zeros(X_train.shape[1]), args = (X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.5359871239784"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_zeros.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.44077327e-03, -1.17657070e+00, -1.79425566e-01,  1.31235577e-02,\n",
       "       -1.93704398e+00,  3.02578604e-03, -2.87979257e-03,  4.19184864e+00,\n",
       "       -4.27623622e-01,  8.74711330e-01,  3.00696798e-01])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_zeros.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.44075295e-03, -1.17657063e+00, -1.79425396e-01,  1.31235602e-02,\n",
       "       -1.93704483e+00,  3.02578642e-03, -2.87979293e-03,  4.19184915e+00,\n",
       "       -4.27623733e-01,  8.74711508e-01,  3.00696789e-01])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_large = minimize(RSS, x0= np.random.normal(0,1000,X_train.shape[1]), args = (X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.5359871239339"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_large.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.44077651e-03, -1.17657067e+00, -1.79425602e-01,  1.31235596e-02,\n",
       "       -1.93704369e+00,  3.02578646e-03, -2.87979236e-03,  4.19184913e+00,\n",
       "       -4.27623845e-01,  8.74711099e-01,  3.00696828e-01])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_large.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solver returns the same results (both RSS and the Beta_hat) for a very small initial values(zeros) and very large initial values(normal distribution with a large standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Different Solver Choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_nelder_mead = minimize(RSS, x0= np.random.normal(0,1,X_train.shape[1]), args = (X_train, y_train), method ='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663.0525707160349"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_nelder_mead.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.85541334e-01, -1.24567257e+00, -1.22959360e+00, -8.11333468e-03,\n",
       "       -1.62813603e-01,  4.46845091e-03,  4.81017690e-04, -2.12362156e-01,\n",
       "       -3.63906195e-01,  1.25649638e+00,  5.38135537e-01])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_nelder_mead.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_powell = minimize(RSS, x0= np.random.normal(0,1,X_train.shape[1]), args = (X_train, y_train), method ='Powell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548.0188468930614"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_powell.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.29905277e-02, -1.17590807e+00, -2.58901118e-01,  6.18750697e-03,\n",
       "       -1.52244470e+00,  2.86498584e-03, -2.35266273e-03,  3.52686126e+00,\n",
       "       -3.89837390e-01,  7.87289495e-01,  3.44479569e-01])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_powell.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.5359871241537"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###original solution\n",
    "res.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.44073270e-03, -1.17657103e+00, -1.79425880e-01,  1.31235568e-02,\n",
       "       -1.93704189e+00,  3.02578688e-03, -2.87979344e-03,  4.19185168e+00,\n",
       "       -4.27624410e-01,  8.74711011e-01,  3.00696820e-01])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of the solver changes the results. In this case, both solvers were less performant than the original solver(one of BFGS, L-BFGS, SLSQP). However, Powell solver was much closer and the coefficients all have the same direction as the original solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "argmin  \\sum_{i=1}^{N} (y_{i} - B_{o} - \\sum_{j=1}^{p} x_{ij}*B_{j})^2 + \\lambda*\\sum_{j=1}^{p}B_{j}^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(beta, X, y, lambda_):\n",
    "    return ((y - lm_predict(X, beta))**2).sum() + (lambda_)*(beta**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ridge = minimize(ridge, x0= np.random.normal(0,1,X_train.shape[1]), args = (X_train, y_train, lambda_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.41319740e-03, -1.17714032e+00, -1.80838206e-01,  1.31010424e-02,\n",
       "       -1.91843956e+00,  3.00833749e-03, -2.86312350e-03,  4.13865378e+00,\n",
       "       -4.15201443e-01,  8.73960463e-01,  3.00973416e-01])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ridge.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.7711323715528"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ridge.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.44068992e-03, -1.17657061e+00, -1.79425332e-01,  1.31235580e-02,\n",
       "       -1.93704441e+00,  3.02578768e-03, -2.87979400e-03,  4.19185249e+00,\n",
       "       -4.27624549e-01,  8.74711352e-01,  3.00696785e-01])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545.5359871243074"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122.46057009754884"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS(res_ridge.x, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is very little change in the results, RSS and coefficients are still very similar. Also RSS in the test results are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dict()\n",
    "for lambda_ in [0,.5,1,5,10]:\n",
    "    res_ridge = minimize(ridge, x0= np.random.normal(0,1,X_train.shape[1]), args = (X_train, y_train, lambda_))\n",
    "    output[lambda_] = [res_ridge.x, res_ridge.fun,RSS(res_ridge.x, X_test,y_test) ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122.4262801337932,\n",
       " 123.99003656137126,\n",
       " 124.9638166237936,\n",
       " 126.7858957789673,\n",
       " 127.1108642626774)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Test RSS\n",
    "output[0][2], output[.5][2], output[1][2], output[5][2], output[10][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07196888, -0.91380457, -0.07295217,  0.00661469, -0.22748663,\n",
       "        0.00307556, -0.00198147,  0.38902101,  0.38230478,  0.64411446,\n",
       "        0.3377126 ])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.44068992e-03, -1.17657061e+00, -1.79425332e-01,  1.31235580e-02,\n",
       "       -1.93704441e+00,  3.02578768e-03, -2.87979400e-03,  4.19185249e+00,\n",
       "       -4.27624549e-01,  8.74711352e-01,  3.00696785e-01])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output suggests that no L2 regularization minimizes the RSS in the test data. We can additionally observe that with high penalty, L2 at 10, We see much much smaller coefficients relative to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(beta, X, y, lambda_):\n",
    "    return ((y - lm_predict(X, beta))**2).sum() + (lambda_)*(np.abs(beta)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:1013: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0 / (numpy.dot(yk, sk))\n"
     ]
    }
   ],
   "source": [
    "output = dict()\n",
    "for lambda_ in [0,.5,1,5,10,1000]:\n",
    "    res_lasso = minimize(lasso, x0= np.random.normal(0,1,X_train.shape[1]), args = (X_train, y_train, lambda_))\n",
    "    output[lambda_] = [res_lasso.x, res_lasso.fun,RSS(res_lasso.x, X_test,y_test) ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122.42627139599216,\n",
       " 122.68427642359296,\n",
       " 123.01754973961307,\n",
       " 125.36196061061278,\n",
       " 128.59292679448922,\n",
       " 140.26041693857604)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Test RSS\n",
    "output[0][2], output[.5][2], output[1][2], output[5][2], output[10][2], output[1000][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.37079958e-02, -4.60123766e-09, -1.36890025e-09, -3.65388234e-09,\n",
       "       -6.92654235e-09,  4.24027443e-03, -2.62766091e-09, -1.19732848e-08,\n",
       "        2.54860025e-09, -2.76771624e-09,  4.64152887e-01])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1000][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output suggests that no L1 regularization minimizes the RSS in the test data. We can additionally observe that with high penalty, L1 at 1000, We see some coefficients go to almost zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magnitude can absolute affect the regularization. For example, much smaller scaled feature leads to higher valued coefficient. When we do an L2 regularization on such features, they will be penalized higher because of the larger magnitude of the coefficient. This is in a sense unfair to the smaller scale features and could lead to less effective models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
