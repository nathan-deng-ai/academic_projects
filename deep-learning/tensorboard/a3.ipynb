{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOHRLs9LRPI",
        "colab_type": "text"
      },
      "source": [
        "# Please scroll down, starting from Activation Function section, you will see my answers\n",
        "\n",
        "# A3: TensorBoard\n",
        "\n",
        "## About\n",
        "\n",
        "In this assignment, you will design and run experiments to evaluate the impact of a few common parameters (like the choice of activation function, optimizer, and weight initialization strategy) and visualize the results in TensorBoard.\n",
        "\n",
        "The starter code below shows the mechanics of using TensorBoard in Colab. Unlike the previous assignments, a limited amount of starter code is provided.  3c is an extra credit question, it's optional (you can receive full credit on this assignment without submitting it).\n",
        "\n",
        "## Questions\n",
        "\n",
        "### 3a. \n",
        "**Implement ReLU and compare against a previous activation function**.\n",
        "\n",
        "The year is 2010. It is not commonly known that ReLU is a useful alternative to activation functions like Sigmoid or Tanh (nor has ReLU been implemented in the library you're using). Create a DNN to classify MNIST, and provide your own implementation of ReLU (instead of using a built-in method). Design and run an experiment to compare ReLU against other methods, and use TensorBoard to display your results. What differences do you observe, and why?\n",
        "\n",
        "### 3b. \n",
        "\n",
        "**Optimizer and initalizer and soup**.\n",
        "\n",
        "Do optimizers like Momentum or Adam really make a difference? How about different weight initialize strategies (like random normal, or glorot uniform?) Design and run experiments to find out, and use TensorBoard to display your results. What differences do you observe, and why?\n",
        "\n",
        "### 3c. Extra credit (optional)\n",
        "\n",
        "**Demonstrate the vanishing gradient problem**. \n",
        "\n",
        "Implement an especially deep neural network and train it on a simple dataset like MNIST. Choose activation functions, initialization strategies, and an optimizer that are likely to cause this behavior. Produce histograms of activations and gradients at various layers during training. What do you see? Next, adjust the parameters above to correct this behavior. Visualize and compare the results.\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please submit your assignment on CourseWorks by uploading a zip file that includes:\n",
        "\n",
        "* A Jupyter notebook, containing complete code to reproduce your experiments, and saved output showing your results.\n",
        "\n",
        "* A README file (plaintext is fine). This should contain your written conclusions for each question. These can be brief (a couple paragraphs). Try to be specific in your answers (if ReLU outperfoms sigmoid, try to answer why).\n",
        "\n",
        "* Plots / diagrams (.jpgs). Since it is not convenient to save TensorBoard diagrams directly in a Jupyter notebook, you can take screenshots of your plots and submit them along with your Jupyter notebook in a zip file on CourseWorks. Please name your diagrams appropriately, and refer to them names in your notebook.\n",
        "\n",
        "If you are working in Colab, you can prepare your notebook for submission by ensuring that runs end-to-end, then saving and downloading it:\n",
        "\n",
        "1. ```Runtime -> Restart and run all```\n",
        "1. ```File -> Save```\n",
        "1. ```File -> Download.ipynb```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSc6v5ws8wKS",
        "colab_type": "text"
      },
      "source": [
        "## Starter code for TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE5lIlzkEUAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpe_UINKKLDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4sUrkaeKQSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7884e97a-f4b9-4a0c-8664-035a5d2ce608"
      },
      "source": [
        "%load_ext tensorboard "
      ],
      "execution_count": 424,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ehUDr72LLAJY"
      },
      "source": [
        "**Caution**. The following cell will clear the logs directory. If you're running this on your local machine, be careful executing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScjIcVAJKkmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ6aWI_NLm6C",
        "colab_type": "text"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByT9RfkIK4kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2_huYeq80tv",
        "colab_type": "text"
      },
      "source": [
        "## First style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMLofsyiLthK",
        "colab_type": "text"
      },
      "source": [
        "Define a simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHBo5cgTLuis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEDGbBRMI-_",
        "colab_type": "text"
      },
      "source": [
        "Create a logs directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggu_lZRnL6Uf",
        "colab_type": "code",
        "outputId": "9437991e-c982-463d-abf2-a8b39f6d71c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import datetime \n",
        "import os\n",
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20191029-205044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_icHggivMBXe",
        "colab_type": "text"
      },
      "source": [
        "### Run an experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0c6b61bf-1d96-4386-c014-44c18b20bc4a",
        "id": "Nr1lrzGLekqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.0, nesterov=False) \n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"exp1\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=1, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 2.5442 - accuracy: 0.5350 - val_loss: 1.4496 - val_accuracy: 0.7670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8618791c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 429
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6-cL05DN9fR",
        "colab_type": "text"
      },
      "source": [
        "### Run a second experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CQKoHJ1Aek8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a32bb05d-3d8d-47f9-d9a0-0719b385d7d0"
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"exp2\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.9034 - accuracy: 0.8101 - val_loss: 0.4831 - val_accuracy: 0.8805\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4593 - accuracy: 0.8806 - val_loss: 0.4013 - val_accuracy: 0.8940\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.4051 - accuracy: 0.8910 - val_loss: 0.3686 - val_accuracy: 0.9015\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.3784 - accuracy: 0.8967 - val_loss: 0.3497 - val_accuracy: 0.9062\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.3613 - accuracy: 0.9004 - val_loss: 0.3368 - val_accuracy: 0.9091\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.3494 - accuracy: 0.9036 - val_loss: 0.3276 - val_accuracy: 0.9118\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.3402 - accuracy: 0.9056 - val_loss: 0.3206 - val_accuracy: 0.9131\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.3331 - accuracy: 0.9075 - val_loss: 0.3146 - val_accuracy: 0.9150\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.3272 - accuracy: 0.9092 - val_loss: 0.3108 - val_accuracy: 0.9149\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.3222 - accuracy: 0.9101 - val_loss: 0.3068 - val_accuracy: 0.9165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8616f652e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 430
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_pSh18ZOENe",
        "colab_type": "text"
      },
      "source": [
        "### Start TensorBoard and compare the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPLDEq8MSQYH",
        "colab_type": "text"
      },
      "source": [
        "## Second style\n",
        "Using a Subclassed model and a GradientTape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vV0Hz2RV1C5",
        "colab_type": "text"
      },
      "source": [
        "Prepre the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX0umywbPeW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_ds = train_ds.shuffle(60000).batch(32)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZgdwTLLwq1D",
        "colab_type": "text"
      },
      "source": [
        "Define a simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRaR1rtpUgDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    return self.d1(x)\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMFRp7CCTxmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7aIkldyT1S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZzldV1BV3Rs",
        "colab_type": "text"
      },
      "source": [
        "Training and testing routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDHIccXUP_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEUmM7G6V7ae",
        "colab_type": "text"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGrEExdWV84_",
        "colab_type": "code",
        "outputId": "c882f8ee-7ed2-4e45-8c8d-9674efc617cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(log_dir, \"test\"))"
      ],
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20191029-205155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v-gMTCiwuBO",
        "colab_type": "text"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rxTf4CyVKxK",
        "colab_type": "code",
        "outputId": "73da273b-8288-4fb9-a26b-ec228f376a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "EPOCHS = 2\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4652763605117798, Accuracy: 87.80999755859375, Test Loss: 0.3063490390777588, Test Accuracy: 91.63999938964844\n",
            "Epoch 2, Loss: 0.3039413094520569, Accuracy: 91.56666564941406, Test Loss: 0.28554728627204895, Test Accuracy: 92.07999420166016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrXOlQa5VWnq",
        "colab_type": "code",
        "outputId": "f3331759-ec9a-4489-c7d7-641808b52db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%tensorboard --logdir \"$log_dir\""
      ],
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6007\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVhkroZrVWDj",
        "colab_type": "text"
      },
      "source": [
        "# Part 1. Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PROoclUM0Q6H",
        "colab_type": "text"
      },
      "source": [
        "### 3a. \n",
        "**Implement ReLU and compare against a previous activation function**.\n",
        "\n",
        "The year is 2010. It is not commonly known that ReLU is a useful alternative to activation functions like Sigmoid or Tanh (nor has ReLU been implemented in the library you're using). Create a DNN to classify MNIST, and provide your own implementation of ReLU (instead of using a built-in method). Design and run an experiment to compare ReLU against other methods, and use TensorBoard to display your results. What differences do you observe, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuW_TBbO0R6O",
        "colab_type": "text"
      },
      "source": [
        "I will first implement Relu and run 3 trials for the most popular activation functions. \n",
        "\n",
        "1. My implementation of Relu\n",
        "2. Tanh\n",
        "3. Sigmoid\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5YQ0RvEvVidn",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1JiCM6pTVidt"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7K_WTN07Vidt",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5mkf-9CsTqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_ds = train_ds.shuffle(60000).batch(32)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RZDGOOFVqye",
        "colab_type": "text"
      },
      "source": [
        "Define a relu activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9EpRXnRVdg8V",
        "colab": {}
      },
      "source": [
        "def my_relu(x, alpha=0.0, max_value=None, threshold=0.0):\n",
        "  return tf.math.maximum(x,tf.zeros(x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yV_vXGvZdg8a",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(10)\n",
        "    self.d2 = Dense(10)\n",
        "    self.d3 = Dense(10, activation = 'softmax')\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = my_relu(x)\n",
        "    x = self.d2(x)\n",
        "    x = my_relu(x)\n",
        "    return self.d3(x)\n",
        "  \n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kqXNqNnEdVAT",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fyDBrbpEdVAX",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JNKgfOk8dVAZ"
      },
      "source": [
        "Training and testing routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2yTRPKKLdVAa",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fSF0SS1EdVAe"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b2e89401-041b-4446-97b8-9a64be78eea0",
        "id": "rnZryIfldVAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)\n",
        "exp_dir = os.path.join(log_dir, \"relu\")\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"test\"))"
      ],
      "execution_count": 447,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20191029-205209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o7FdrW_xdVAk"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f79758c9-0ac8-4232-bb9b-ecf4a5d85bb5",
        "id": "fQvWTtVNdVAk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 448,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.5306733846664429, Accuracy: 84.66500091552734, Test Loss: 0.32465100288391113, Test Accuracy: 90.58999633789062\n",
            "Epoch 2, Loss: 0.30427291989326477, Accuracy: 91.41999816894531, Test Loss: 0.2798433303833008, Test Accuracy: 92.08999633789062\n",
            "Epoch 3, Loss: 0.2750680446624756, Accuracy: 92.20832824707031, Test Loss: 0.27094700932502747, Test Accuracy: 92.02999877929688\n",
            "Epoch 4, Loss: 0.25651854276657104, Accuracy: 92.66166687011719, Test Loss: 0.2621849775314331, Test Accuracy: 92.22999572753906\n",
            "Epoch 5, Loss: 0.24128377437591553, Accuracy: 93.13999938964844, Test Loss: 0.2505202889442444, Test Accuracy: 92.87999725341797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNzhADGp1DT_",
        "colab_type": "text"
      },
      "source": [
        "Running Tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "62QEVWKS065B",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(10, activation = 'tanh')\n",
        "    self.d2 = Dense(10, activation = 'tanh')\n",
        "    self.d3 = Dense(10, activation = 'softmax')\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    return self.d3(x)\n",
        "  \n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VYnz_BG2065G",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LSPeTABm065H",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSCijyQN1p73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cSJEJg6Q065L"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cwNbei5E065M",
        "colab": {}
      },
      "source": [
        "exp_dir = os.path.join(log_dir, \"tanh\")\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"test\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7cEk1U0h065N"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2bd7b5f4-4b14-4dd1-edf9-e276e4fb39f7",
        "id": "7Mt0c17b065O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 454,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.6247983574867249, Accuracy: 84.37833404541016, Test Loss: 0.35505181550979614, Test Accuracy: 90.4800033569336\n",
            "Epoch 2, Loss: 0.3182063698768616, Accuracy: 91.24166107177734, Test Loss: 0.290274053812027, Test Accuracy: 92.0999984741211\n",
            "Epoch 3, Loss: 0.27654829621315, Accuracy: 92.22166442871094, Test Loss: 0.2810467481613159, Test Accuracy: 92.18000030517578\n",
            "Epoch 4, Loss: 0.2588479518890381, Accuracy: 92.66666412353516, Test Loss: 0.2705027759075165, Test Accuracy: 92.27999877929688\n",
            "Epoch 5, Loss: 0.24592912197113037, Accuracy: 92.90166473388672, Test Loss: 0.26797130703926086, Test Accuracy: 92.36000061035156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBiJRTIB1Rcm",
        "colab_type": "text"
      },
      "source": [
        "Running Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ojhoESi1VzS",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(10, activation = 'sigmoid')\n",
        "    self.d2 = Dense(10, activation = 'sigmoid')\n",
        "    self.d3 = Dense(10, activation = 'softmax')\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    return self.d3(x)\n",
        "  \n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cY23cnNf1VzU",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YiNCtSY41VzX",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejW4UfvT1uPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X9HhwDon1Vza"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1i45RH-y1Vzb",
        "colab": {}
      },
      "source": [
        "exp_dir = os.path.join(log_dir, \"sigmoid\")\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"train_sigmoid\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"test_sigmoid\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ndeVDFbP1Vzd"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a2e623fb-acf7-4bb0-9adf-6a68563bcea6",
        "id": "gv5cV1eR1Vzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 460,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.4744197130203247, Accuracy: 62.17333221435547, Test Loss: 0.9238422513008118, Test Accuracy: 81.55000305175781\n",
            "Epoch 2, Loss: 0.7018621563911438, Accuracy: 85.23332977294922, Test Loss: 0.5482260584831238, Test Accuracy: 87.12999725341797\n",
            "Epoch 3, Loss: 0.4684363305568695, Accuracy: 88.72000122070312, Test Loss: 0.41762325167655945, Test Accuracy: 89.24000549316406\n",
            "Epoch 4, Loss: 0.3746112287044525, Accuracy: 90.21666717529297, Test Loss: 0.36158299446105957, Test Accuracy: 89.9000015258789\n",
            "Epoch 5, Loss: 0.32900744676589966, Accuracy: 91.0816650390625, Test Loss: 0.332383394241333, Test Accuracy: 90.83999633789062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHu6MCZphh6m",
        "colab_type": "text"
      },
      "source": [
        "Running Relu(built-in)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2cZ0XGX0hljG",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(10, activation = 'relu')\n",
        "    self.d2 = Dense(10, activation = 'relu')\n",
        "    self.d3 = Dense(10, activation = 'softmax')\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    return self.d3(x)\n",
        "  \n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SyaW1OqwhljK",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E7gTOQ-UhljM",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6cMTVlHrhljO",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zlICg_uLhljQ"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8AwqkUBVhljQ",
        "colab": {}
      },
      "source": [
        "exp_dir = os.path.join(log_dir, \"relu(built-in)\")\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"train_sigmoid\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"test_sigmoid\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xqnGHcG1hljS"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8326ef42-f28f-4ba6-cc62-5dbb145dfc1c",
        "id": "pMBdE0lphljT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "\n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = tf.random.normal((32, 100))\n",
        "    tf.summary.histogram('random', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Your description')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.6467837691307068, Accuracy: 80.83833312988281, Test Loss: 0.34676527976989746, Test Accuracy: 90.31999969482422\n",
            "Epoch 2, Loss: 0.31481799483299255, Accuracy: 90.99666595458984, Test Loss: 0.26775500178337097, Test Accuracy: 92.08999633789062\n",
            "Epoch 3, Loss: 0.27290764451026917, Accuracy: 92.20999908447266, Test Loss: 0.24902066588401794, Test Accuracy: 92.55999755859375\n",
            "Epoch 4, Loss: 0.25029149651527405, Accuracy: 92.79666900634766, Test Loss: 0.2345554530620575, Test Accuracy: 93.0999984741211\n",
            "Epoch 5, Loss: 0.23757301270961761, Accuracy: 93.07500457763672, Test Loss: 0.2299991399049759, Test Accuracy: 93.27999877929688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6a5c2a77-cb7f-4518-e962-7bc32fe7a79b",
        "id": "VJDwkoXtdON4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%tensorboard --logdir \"$log_dir\""
      ],
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6008\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEFa5MludeIx",
        "colab_type": "text"
      },
      "source": [
        "# Part 2.a Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiGpbs07rZvD",
        "colab_type": "text"
      },
      "source": [
        "**Optimizer and initalizer and soup**.\n",
        "\n",
        "Do optimizers like Momentum or Adam really make a difference? How about different weight initialize strategies (like random normal, or glorot uniform?) Design and run experiments to find out, and use TensorBoard to display your results. What differences do you observe, and why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rbTmUtLhqvA_",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bumvx9dgqvBC"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_QAfV7IqvBD",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t5EybJXqvBF"
      },
      "source": [
        "Define a relu activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-fBKLPjrARq",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(10, activation = 'relu'),\n",
        "    Dense(10, activation = 'relu'),\n",
        "    Dense(10, activation='softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3sbkmO1KrARs"
      },
      "source": [
        "Create a logs directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0d64bc35-1a42-4194-d847-7aaa2bdcbf2d",
        "id": "tPkWyOmwrARt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 471,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20191029-205346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jA7_z6m6rARu"
      },
      "source": [
        "### Run an experiment with SGD solver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmA_vkUc-l1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD, Adam, Adadelta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "37799e33-59c2-48f0-bff1-f737c2f9e918",
        "id": "TpKQfKvyrARv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.0, nesterov=False) \n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"SGD\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 473,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 2.1875 - accuracy: 0.2463 - val_loss: 2.0310 - val_accuracy: 0.3107\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 1.8486 - accuracy: 0.3648 - val_loss: 1.6562 - val_accuracy: 0.4103\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 1.4554 - accuracy: 0.5484 - val_loss: 1.2551 - val_accuracy: 0.6456\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 1.1194 - accuracy: 0.6882 - val_loss: 0.9960 - val_accuracy: 0.7235\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.9201 - accuracy: 0.7404 - val_loss: 0.8425 - val_accuracy: 0.7609\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.7953 - accuracy: 0.7710 - val_loss: 0.7412 - val_accuracy: 0.7861\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.7095 - accuracy: 0.7945 - val_loss: 0.6681 - val_accuracy: 0.8048\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.6479 - accuracy: 0.8115 - val_loss: 0.6150 - val_accuracy: 0.8188\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.6021 - accuracy: 0.8238 - val_loss: 0.5761 - val_accuracy: 0.8308\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.5673 - accuracy: 0.8341 - val_loss: 0.5446 - val_accuracy: 0.8419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8627eecc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 473
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ovh9ocNxrAR2"
      },
      "source": [
        "### Run a second experiment with SGD solver again but with higher momentum "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "42a38063-96dd-4436-ee1a-069d4b356c14",
        "id": "K_NpBdt8rAR3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"SGD, higher momentum\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 474,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 124us/sample - loss: 0.8959 - accuracy: 0.7251 - val_loss: 0.4169 - val_accuracy: 0.8787\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.3869 - accuracy: 0.8898 - val_loss: 0.3340 - val_accuracy: 0.9026\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.3342 - accuracy: 0.9052 - val_loss: 0.3073 - val_accuracy: 0.9124\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.3100 - accuracy: 0.9123 - val_loss: 0.2907 - val_accuracy: 0.9191\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.2923 - accuracy: 0.9171 - val_loss: 0.2797 - val_accuracy: 0.9208\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 0.2792 - accuracy: 0.9195 - val_loss: 0.2732 - val_accuracy: 0.9226\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.2671 - accuracy: 0.9228 - val_loss: 0.2574 - val_accuracy: 0.9243\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.2566 - accuracy: 0.9257 - val_loss: 0.2515 - val_accuracy: 0.9295\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.2475 - accuracy: 0.9295 - val_loss: 0.2475 - val_accuracy: 0.9296\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 8s 128us/sample - loss: 0.2404 - accuracy: 0.9312 - val_loss: 0.2347 - val_accuracy: 0.9329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8624f599e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 474
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPRy95yq-RIf",
        "colab_type": "text"
      },
      "source": [
        "### Run a Third Experiment with Adam Solver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H4bkVPX-gKi",
        "colab_type": "code",
        "outputId": "e441f72c-3d6f-45c4-9da4-65b25c35752a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"Adam\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 0.6209 - accuracy: 0.8201 - val_loss: 0.3751 - val_accuracy: 0.8944\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.3226 - accuracy: 0.9072 - val_loss: 0.2778 - val_accuracy: 0.9228\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.2660 - accuracy: 0.9244 - val_loss: 0.2547 - val_accuracy: 0.9281\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.2417 - accuracy: 0.9305 - val_loss: 0.2331 - val_accuracy: 0.9345\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 0.2266 - accuracy: 0.9347 - val_loss: 0.2472 - val_accuracy: 0.9257\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2166 - accuracy: 0.9385 - val_loss: 0.2424 - val_accuracy: 0.9327\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.2094 - accuracy: 0.9402 - val_loss: 0.2123 - val_accuracy: 0.9386\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.2017 - accuracy: 0.9427 - val_loss: 0.2137 - val_accuracy: 0.9399\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 0.1956 - accuracy: 0.9440 - val_loss: 0.2243 - val_accuracy: 0.9342\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.1900 - accuracy: 0.9453 - val_loss: 0.2172 - val_accuracy: 0.9362\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f861b9cd940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 475
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwe6jM9A_Em3",
        "colab_type": "text"
      },
      "source": [
        "### Run a Fourth Experiment with AdaDelta Solver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfnOYqcG-_je",
        "colab_type": "code",
        "outputId": "0c1be878-408a-401d-f291-bd614139eeff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = Adadelta(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"AdaDelta\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 2.2989 - accuracy: 0.1023 - val_loss: 2.2916 - val_accuracy: 0.1121\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 2.2848 - accuracy: 0.1225 - val_loss: 2.2781 - val_accuracy: 0.1325\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 2.2701 - accuracy: 0.1400 - val_loss: 2.2624 - val_accuracy: 0.1478\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 2.2537 - accuracy: 0.1531 - val_loss: 2.2451 - val_accuracy: 0.1599\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 122us/sample - loss: 2.2357 - accuracy: 0.1641 - val_loss: 2.2265 - val_accuracy: 0.1707\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 2.2169 - accuracy: 0.1737 - val_loss: 2.2075 - val_accuracy: 0.1787\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 2.1977 - accuracy: 0.1827 - val_loss: 2.1883 - val_accuracy: 0.1895\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 2.1788 - accuracy: 0.1930 - val_loss: 2.1694 - val_accuracy: 0.2008\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 2.1602 - accuracy: 0.2033 - val_loss: 2.1510 - val_accuracy: 0.2103\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 2.1421 - accuracy: 0.2130 - val_loss: 2.1329 - val_accuracy: 0.2229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8613108358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FtuQCxLZrAR4"
      },
      "source": [
        "### Start TensorBoard and compare the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a2865edb-02f2-46c0-e2ad-cd89ee4aae6a",
        "id": "SBOf35vYrAR5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%tensorboard --logdir \"$log_dir\""
      ],
      "execution_count": 477,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6009\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UHDGyp7_S3J",
        "colab_type": "text"
      },
      "source": [
        "### Part 2.b Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jry3efRpA4sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.initializers import Ones, Zeros, RandomNormal, GlorotUniform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkyrxFzl_ZdE",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6CDcE6t8_ZdI"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OK0CkWVM_ZdJ",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Iz9YkHtS_ZdK"
      },
      "source": [
        "Define a relu activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wPauy4yu_ZdL",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      \n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(10, activation = 'relu', kernel_initializer = Ones()),\n",
        "    Dense(10, activation = 'relu'),\n",
        "    Dense(10, activation='softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ml01sg_j_ZdN"
      },
      "source": [
        "Create a logs directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a0eb6fbc-93f3-4077-a2a2-6a47f998abb0",
        "id": "ZkMYwHld_ZdN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 482,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20191029-205837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfC--jsG_ZdS"
      },
      "source": [
        "### Run an experiment with ones as initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "336e091e-641a-4622-91e9-d7144f8d8e7c",
        "id": "86nHIVyY_ZdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.9, nesterov=True) \n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"ones\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 128us/sample - loss: 2.5023 - accuracy: 0.1112 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f86056b6c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 483
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ynmx63AG_ZdW"
      },
      "source": [
        "### Run a second experiment with glorot uniform as initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ4itj94BdNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      \n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(10, activation = 'relu', kernel_initializer = GlorotUniform()),\n",
        "    Dense(10, activation = 'relu'),\n",
        "    Dense(10, activation='softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "76cf1da2-7442-4f9a-e6e0-21c649d076f9",
        "id": "7QNKa1yy_ZdW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"glorot\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 485,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 123us/sample - loss: 1.0507 - accuracy: 0.6625 - val_loss: 0.4864 - val_accuracy: 0.8602\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.4412 - accuracy: 0.8741 - val_loss: 0.3814 - val_accuracy: 0.8909\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.3649 - accuracy: 0.8963 - val_loss: 0.3343 - val_accuracy: 0.9027\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.3259 - accuracy: 0.9075 - val_loss: 0.3174 - val_accuracy: 0.9083\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.3021 - accuracy: 0.9131 - val_loss: 0.2929 - val_accuracy: 0.9147\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.2854 - accuracy: 0.9179 - val_loss: 0.2855 - val_accuracy: 0.9170\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.2736 - accuracy: 0.9218 - val_loss: 0.2709 - val_accuracy: 0.9205\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 0.2640 - accuracy: 0.9248 - val_loss: 0.2625 - val_accuracy: 0.9244\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.2562 - accuracy: 0.9265 - val_loss: 0.2578 - val_accuracy: 0.9232\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.2500 - accuracy: 0.9279 - val_loss: 0.2504 - val_accuracy: 0.9269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f860494ad30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 485
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VqaEDQy3BjFA"
      },
      "source": [
        "### Run a third experiment with zeros as initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iGMGQoPZBjFG",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      \n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(10, activation = 'relu', kernel_initializer = Zeros()),\n",
        "    Dense(10, activation = 'relu'),\n",
        "    Dense(10, activation='softmax'),\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O0XY6x1PBjFI",
        "outputId": "2ad3498f-17d5-4cc9-bc54-b02b85cf4d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"zeros\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=exp_dir)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 487,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 126us/sample - loss: 2.3016 - accuracy: 0.1120 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 124us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 8s 129us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8602b69d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 487
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puZwKuL3_Qu1",
        "colab_type": "code",
        "outputId": "f38ee0d7-f81f-4a70-b986-3667bfb9b2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%tensorboard --logdir \"$log_dir\""
      ],
      "execution_count": 488,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6010\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC7aTNxynXeC",
        "colab_type": "text"
      },
      "source": [
        "### 3c. Extra credit (optional)\n",
        "\n",
        "**Demonstrate the vanishing gradient problem**. \n",
        "\n",
        "Implement an especially deep neural network and train it on a simple dataset like MNIST. Choose activation functions, initialization strategies, and an optimizer that are likely to cause this behavior. Produce histograms of activations and gradients at various layers during training. What do you see? Next, adjust the parameters above to correct this behavior. Visualize and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9E1ao3jn9Mb",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NR09_Oj_n9Me"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbIEklzPn9Me",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dOyoHScWn9Mi",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      \n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(100, activation = 'sigmoid'),\n",
        "    Dense(10, activation='softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_4kd_lljn9Mk"
      },
      "source": [
        "Create a logs directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "25e2c1eb-5737-42bf-d247-5d7ed0146265",
        "id": "V4-EAR0_n9Ml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)"
      ],
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20191029-210216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "udKWm7eLn9Mo"
      },
      "source": [
        "### Run an experiment with a very deep NN with Sigmoid Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a319b359-1193-45d8-c63c-f597ed39adfc",
        "id": "vwCAcAMdn9Mp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.00, nesterov=False) \n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"sigmoid_hist\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir = exp_dir, histogram_freq=1, write_graph=True, write_images=True, write_grads = True)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 493,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "   32/60000 [..............................] - ETA: 17:35 - loss: 2.7394 - accuracy: 0.1250WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.139085). Check your callbacks.\n",
            "60000/60000 [==============================] - 11s 178us/sample - loss: 2.3383 - accuracy: 0.1067 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 2.3014 - accuracy: 0.1121 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 9s 155us/sample - loss: 2.3014 - accuracy: 0.1122 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 9s 158us/sample - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 9s 155us/sample - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 9s 155us/sample - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f85fe7e7ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 493
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5cKTTvinI7ph"
      },
      "source": [
        "### Experiment 2. Use Relu Activation with a very deep NN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1yxdXjkQI7pi",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      \n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GoPC0vSII7pn"
      },
      "source": [
        "### Run an experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "13447c56-1cf5-412f-f862-254f2599cdc6",
        "id": "XhI79JArI7pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "model = create_model() \n",
        "opt = SGD(learning_rate=0.001, momentum=0.00, nesterov=False) \n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "exp_dir = os.path.join(log_dir, \"relu_hist\")\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir = exp_dir, histogram_freq=1, write_graph=True, write_images=True, write_grads = True)\n",
        "\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=10, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tb_callback])"
      ],
      "execution_count": 495,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "   32/60000 [..............................] - ETA: 16:48 - loss: 2.3050 - accuracy: 0.0625WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.114789). Check your callbacks.\n",
            "60000/60000 [==============================] - 11s 179us/sample - loss: 2.3009 - accuracy: 0.1227 - val_loss: 2.2980 - val_accuracy: 0.1330\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 2.2957 - accuracy: 0.1278 - val_loss: 2.2924 - val_accuracy: 0.1364\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 161us/sample - loss: 2.2888 - accuracy: 0.1455 - val_loss: 2.2826 - val_accuracy: 0.1810\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 161us/sample - loss: 2.2740 - accuracy: 0.2218 - val_loss: 2.2591 - val_accuracy: 0.3050\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 157us/sample - loss: 2.2309 - accuracy: 0.3541 - val_loss: 2.1789 - val_accuracy: 0.4068\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 9s 156us/sample - loss: 2.0032 - accuracy: 0.4039 - val_loss: 1.6403 - val_accuracy: 0.5145\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 9s 157us/sample - loss: 1.0220 - accuracy: 0.7048 - val_loss: 0.6763 - val_accuracy: 0.7908\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 160us/sample - loss: 0.6129 - accuracy: 0.8112 - val_loss: 0.5326 - val_accuracy: 0.8347\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 158us/sample - loss: 0.5184 - accuracy: 0.8424 - val_loss: 0.4690 - val_accuracy: 0.8611\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 9s 156us/sample - loss: 0.4567 - accuracy: 0.8641 - val_loss: 0.4262 - val_accuracy: 0.8758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f85fcce1b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 495
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxF-y9Ouqvhe",
        "colab_type": "code",
        "outputId": "3971fb76-4913-4762-c619-be543a957c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%tensorboard --logdir \"$log_dir\""
      ],
      "execution_count": 496,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6011\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSa5IvzSO4OJ",
        "colab_type": "text"
      },
      "source": [
        "### 3.3b Observe Gradients Starting with Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSOrX1C242W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./tensorboard-logs/ # Clear any logs from previous runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxMtbAbm5do7"
      },
      "source": [
        "Import a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SKc-6GFf5do8",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cVHxjrY_5dpB",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_ds = train_ds.shuffle(60000).batch(32)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTxMd0OQ5dpI",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(100, activation = 'sigmoid')\n",
        "    self.d2 = Dense(100, activation = 'sigmoid')\n",
        "    self.d3 = Dense(100, activation = 'sigmoid')\n",
        "    self.d4 = Dense(100, activation = 'sigmoid')\n",
        "    self.d5 = Dense(100, activation = 'sigmoid')\n",
        "    self.d6 = Dense(100, activation = 'sigmoid')\n",
        "    self.d7 = Dense(100, activation = 'sigmoid')\n",
        "    self.d8 = Dense(100, activation = 'sigmoid')\n",
        "    self.d9 = Dense(100, activation = 'sigmoid')\n",
        "    self.d10 = Dense(100, activation = 'sigmoid')\n",
        "    self.d11 = Dense(100, activation = 'sigmoid')\n",
        "    self.d12 = Dense(100, activation = 'sigmoid')\n",
        "    self.d13 = Dense(100, activation = 'sigmoid')\n",
        "    self.d14 = Dense(100, activation = 'sigmoid')\n",
        "    self.d15 = Dense(100, activation = 'sigmoid')\n",
        "    self.d16 = Dense(100, activation = 'sigmoid')\n",
        "    self.d17 = Dense(100, activation = 'sigmoid')\n",
        "    self.d18 = Dense(100, activation = 'sigmoid')\n",
        "    self.d19 = Dense(100, activation = 'sigmoid')\n",
        "    self.d20 = Dense(100, activation = 'sigmoid')\n",
        "    self.d21 = Dense(10, activation = 'softmax')\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.d3(x)\n",
        "    x = self.d4(x)\n",
        "    x = self.d5(x)\n",
        "    x = self.d6(x)\n",
        "    x = self.d7(x)\n",
        "    x = self.d8(x)\n",
        "    x = self.d9(x)\n",
        "    x = self.d10(x)\n",
        "    x = self.d11(x)\n",
        "    x = self.d12(x)\n",
        "    x = self.d13(x)\n",
        "    x = self.d14(x)\n",
        "    x = self.d15(x)\n",
        "    x = self.d16(x)\n",
        "    x = self.d17(x)\n",
        "    x = self.d18(x)\n",
        "    x = self.d19(x)\n",
        "    x = self.d20(x)\n",
        "    return self.d21(x)\n",
        "  \n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cu6fY2_o5dpL",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zXEdv49U5dpP",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "gradient = tf.keras.metrics.Mean(name='gradient')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9DYuL3sR5dpS"
      },
      "source": [
        "Training and testing routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KPFWEQtq5dpU",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  gradients_list.append(gradients)\n",
        "  gradient(gradients[1])\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "19rFi3fz5dpV"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4a55b5d3-9533-4c23-c4ff-0c6cea3c1278",
        "id": "Y_tKJ8GC5dpW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = os.path.join(\"./tensorboard-logs/\", date)\n",
        "print(\"Writing logs to\", log_dir)\n",
        "exp_dir = os.path.join(log_dir, \"sigmoid\")\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"test\"))"
      ],
      "execution_count": 504,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing logs to ./tensorboard-logs/20191029-210549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tDJMJ3qw5dpY"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8ad80285-bf9b-42d4-9793-a2c526c7522d",
        "id": "VAMDPN1p5dpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "gradients_list_sigmoid = list()\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, Gradient: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100,\n",
        "                        gradient.result()))\n",
        "  gradients_list_sigmoid.append(gradient.result())\n",
        "  \n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    \n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = gradient.result()\n",
        "    tf.summary.histogram('gradient', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Gradient')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 505,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 2.3107540607452393, Accuracy: 10.220000267028809, Test Loss: 2.3073275089263916, Test Accuracy: 11.350000381469727, Gradient: -7.788556458328835e-18\n",
            "Epoch 2, Loss: 2.3073623180389404, Accuracy: 10.460000038146973, Test Loss: 2.308603048324585, Test Accuracy: 11.350000381469727, Gradient: -4.717657533212458e-18\n",
            "Epoch 3, Loss: 2.304349184036255, Accuracy: 10.633333206176758, Test Loss: 2.301701545715332, Test Accuracy: 11.350000381469727, Gradient: -3.306694410145284e-18\n",
            "Epoch 4, Loss: 2.3019251823425293, Accuracy: 11.065000534057617, Test Loss: 2.3013241291046143, Test Accuracy: 11.350000381469727, Gradient: -2.55932597335566e-18\n",
            "Epoch 5, Loss: 2.301515579223633, Accuracy: 11.208333015441895, Test Loss: 2.301358938217163, Test Accuracy: 11.350000381469727, Gradient: -2.055819480133407e-18\n",
            "Epoch 6, Loss: 2.3014285564422607, Accuracy: 11.236666679382324, Test Loss: 2.3012146949768066, Test Accuracy: 11.350000381469727, Gradient: -1.7150237560985505e-18\n",
            "Epoch 7, Loss: 2.301405191421509, Accuracy: 11.236666679382324, Test Loss: 2.3009657859802246, Test Accuracy: 11.350000381469727, Gradient: -1.4707596993559025e-18\n",
            "Epoch 8, Loss: 2.30133318901062, Accuracy: 11.236666679382324, Test Loss: 2.301089286804199, Test Accuracy: 11.350000381469727, Gradient: -1.2870626342449997e-18\n",
            "Epoch 9, Loss: 2.3013620376586914, Accuracy: 11.236666679382324, Test Loss: 2.301015615463257, Test Accuracy: 11.350000381469727, Gradient: -1.144233461273045e-18\n",
            "Epoch 10, Loss: 2.3013665676116943, Accuracy: 11.236666679382324, Test Loss: 2.3011577129364014, Test Accuracy: 11.350000381469727, Gradient: -1.0298309704369345e-18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnjbiFQr6CeS",
        "colab_type": "text"
      },
      "source": [
        "### Run Second Experiment on Relu for Gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NBw2UIhQ6BuD",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ia9u1zPf6BuI",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_ds = train_ds.shuffle(60000).batch(32)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d93EyCuN6BuJ",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten(input_shape=(28, 28))\n",
        "    self.d1 = Dense(100, activation = 'relu')\n",
        "    self.d2 = Dense(100, activation = 'relu')\n",
        "    self.d3 = Dense(100, activation = 'relu')\n",
        "    self.d4 = Dense(100, activation = 'relu')\n",
        "    self.d5 = Dense(100, activation = 'relu')\n",
        "    self.d6 = Dense(100, activation = 'relu')\n",
        "    self.d7 = Dense(100, activation = 'relu')\n",
        "    self.d8 = Dense(100, activation = 'relu')\n",
        "    self.d9 = Dense(100, activation = 'relu')\n",
        "    self.d10 = Dense(100, activation = 'relu')\n",
        "    self.d11 = Dense(100, activation = 'relu')\n",
        "    self.d12 = Dense(100, activation = 'relu')\n",
        "    self.d13 = Dense(100, activation = 'relu')\n",
        "    self.d14 = Dense(100, activation = 'relu')\n",
        "    self.d15 = Dense(100, activation = 'relu')\n",
        "    self.d16 = Dense(100, activation = 'relu')\n",
        "    self.d17 = Dense(100, activation = 'relu')\n",
        "    self.d18 = Dense(100, activation = 'relu')\n",
        "    self.d19 = Dense(100, activation = 'relu')\n",
        "    self.d20 = Dense(100, activation = 'relu')\n",
        "    self.d21 = Dense(100, activation = 'softmax')\n",
        "    \n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.d3(x)\n",
        "    x = self.d4(x)\n",
        "    x = self.d5(x)\n",
        "    x = self.d6(x)\n",
        "    x = self.d7(x)\n",
        "    x = self.d8(x)\n",
        "    x = self.d9(x)\n",
        "    x = self.d10(x)\n",
        "    x = self.d11(x)\n",
        "    x = self.d12(x)\n",
        "    x = self.d13(x)\n",
        "    x = self.d14(x)\n",
        "    x = self.d15(x)\n",
        "    x = self.d16(x)\n",
        "    x = self.d17(x)\n",
        "    x = self.d18(x)\n",
        "    x = self.d19(x)\n",
        "    x = self.d20(x)\n",
        "    return self.d21(x)\n",
        "  \n",
        "model = MyModel()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iU7-E3Vc6BuK",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JhWdJmz6BuM",
        "colab": {}
      },
      "source": [
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "gradient = tf.keras.metrics.Mean(name='gradient')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nWC4sYrM6BuN"
      },
      "source": [
        "Training and testing routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0GUw2hBE6BuN",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  gradients_list.append(gradients)\n",
        "  gradient(gradients[1])\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rTsz9Nkl6BuP"
      },
      "source": [
        "Prepare log writers (previously, these were handled by the callback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DIZOmwtR6BuQ",
        "colab": {}
      },
      "source": [
        "exp_dir = os.path.join(log_dir, \"relu\")\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(exp_dir, \"test\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sj3-lCqv6BuS"
      },
      "source": [
        "Train and log summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "95779c14-c36b-4431-bd82-38561d96d8c9",
        "id": "6UivZjOS6BuS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "gradients_list_relu = list()\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "    \n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, Gradient: {}'\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100,\n",
        "                        gradient.result()))\n",
        "  \n",
        "  gradients_list_relu.append(gradient.result())\n",
        "  with train_writer.as_default():\n",
        "    tf.summary.scalar('gradient', gradient.result(), step=epoch)\n",
        "    \n",
        "    \n",
        "    # ====\n",
        "    # Demo: show how to use histogram summaries\n",
        "    # Create and log some random data\n",
        "    # Useful if you're attemping the extra credit question\n",
        "    # ====\n",
        "    data = gradient.result()\n",
        "    tf.summary.histogram('gradient', \n",
        "                         data,\n",
        "                         step=epoch, \n",
        "                         description='Gradient')\n",
        "    \n",
        "  with test_writer.as_default():\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    \n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 513,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.0104414224624634, Accuracy: 63.7833366394043, Test Loss: 0.4007866680622101, Test Accuracy: 88.62000274658203, Gradient: 0.00013746661716140807\n",
            "Epoch 2, Loss: 0.3316991627216339, Accuracy: 91.09667205810547, Test Loss: 0.25157395005226135, Test Accuracy: 93.58000183105469, Gradient: 9.528292139293626e-05\n",
            "Epoch 3, Loss: 0.267437607049942, Accuracy: 93.02999877929688, Test Loss: 0.2687624990940094, Test Accuracy: 93.66000366210938, Gradient: 7.013735012151301e-05\n",
            "Epoch 4, Loss: 0.24027717113494873, Accuracy: 93.76499938964844, Test Loss: 0.19441461563110352, Test Accuracy: 95.02000427246094, Gradient: 0.00018885750614572316\n",
            "Epoch 5, Loss: 0.18163403868675232, Accuracy: 95.40167236328125, Test Loss: 0.18364568054676056, Test Accuracy: 95.44000244140625, Gradient: 0.0001584183773957193\n",
            "Epoch 6, Loss: 0.16513343155384064, Accuracy: 95.92499542236328, Test Loss: 0.16160379350185394, Test Accuracy: 96.05000305175781, Gradient: 0.00013911932182963938\n",
            "Epoch 7, Loss: 0.15827204287052155, Accuracy: 96.17833709716797, Test Loss: 0.21747812628746033, Test Accuracy: 95.34000396728516, Gradient: 0.00012586796947289258\n",
            "Epoch 8, Loss: 0.15024560689926147, Accuracy: 96.31500244140625, Test Loss: 0.17256008088588715, Test Accuracy: 95.6500015258789, Gradient: 0.00011369051208021119\n",
            "Epoch 9, Loss: 0.13790388405323029, Accuracy: 96.63333129882812, Test Loss: 0.2049408257007599, Test Accuracy: 95.25, Gradient: 0.00010796733113238588\n",
            "Epoch 10, Loss: 0.14384305477142334, Accuracy: 96.413330078125, Test Loss: 0.1421331763267517, Test Accuracy: 96.62000274658203, Gradient: 8.51922741276212e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13E6oP95LeJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "relu_grads = [i.numpy() for i in gradients_list_relu]\n",
        "sigmoid_grads = [i.numpy() for i in gradients_list_sigmoid]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y5x5qaFLeEr",
        "colab_type": "code",
        "outputId": "9498d904-bdb1-497f-ba01-3a8feeafaa5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(np.linspace(1,10,10),relu_grads)\n",
        "plt.title('Average Gradients of Second Layer using Relu')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Average Gradients')"
      ],
      "execution_count": 515,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Average Gradients')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 515
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE3CWbIwxxgScJRtjjDEB9/8B3qGRJeCmhGQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJw9wAokOEeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "db509d26-ac4d-40bc-a0db-99832512fe2e"
      },
      "source": [
        "plt.plot(np.linspace(1,10,10),sigmoid_grads)\n",
        "plt.title('Average Gradients of Second Layer using Sigmoid')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Average Gradients')"
      ],
      "execution_count": 516,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Average Gradients')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 516
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYlwAQesATfK",
        "colab_type": "code",
        "outputId": "503c0bb0-1dfa-4f85-8f6b-17f2cb265d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%tensorboard --logdir \"$log_dir\""
      ],
      "execution_count": 517,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6012\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}