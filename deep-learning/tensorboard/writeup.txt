3.1 Activation Functions
Please refer to the plot 3.1,ActivatinFunctions.jpg.
Here we notice a few things, tanh has the best performance in both test and training in terms of accuracy followed by built-in relu, sigmoid then our implmentation of relu.
Our Relu seems to perform very poorly although it does improve over epochs. It's likely we are missing a few optimization operations in our implmentation.
We only implemented the most basic relu, max(0,x) and tensorflow probably optimized it a lot better than that. We can see that the tensorflow's Relu is very competitive to Tanh.
In general, while this is not always the case, relu is the preferred activation function because it does not suffer from vanishing and explosion of gradients and therefore 
it converges faster.

Tanh function performing better than sigmoid makes sense since sigmoid function's derivative maxes out at .25 while it maxes out at 1 for tanh.
This means that, algorithm can make updates on the variables much faster with the tanh function than the sigmoid function. 


3.2.a Optimizer
We first observe that Adam converges the fastest, achieving very high accuracy right from the early epochs. SGD soon catches up. Adadelta however doesn't seem to perform well.
Adam is considered a optimizer that converges fast and is efficient while SGD is an optimizer that is believed to generalize better(https://arxiv.org/abs/1705.08292). This is apparent
in our findings here because SGD slowly catches up to Adam. It is possible that with the right parameter tuning and enough epochs that SGD may have started outperforming Adam.

Adadelta performing the worst makes sense since it is thought to be the solver that works well with sparse data such as NLP and TFIDF problems because it penalizes 
parameters that update frequently, and therefore emphasizing information that is not as frequent. So in our case, this is not a great solver, and it shows in our plots.

Last thing to point out is that SGD with higher momentum performs better than SGD with no momentum. This is because momentum hyperparamter accelerates SGD in the
relevant direction and dampens oscillations. So this higher momentum is helping us converge faster to the optimum. 

3.2.b. Initialization

We try three different initialization settings, ones, zeros and GlorotUniform. We see that GlorotUniform clearly outperforms the other two. This can be due to couple of reasons.
In general, it is not a good idea to initalize the NN weights with equal constants. First, the network can get stuck in a local minima because error is back propagagted
proportion to the values of the weights. By giving lots of different values, it is less likely to get stuck in a local minima and gives us a better chance of finding
the global minimum. 

Additionally, when neurons all start with the same weights, then they all follow the same gradient. Which means that they cannot learn different things together. This makes
them less optimal in scenarios where they need to be learning different things.

Last two points help us explain why GlorotUniform works the best. It samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / (fan_in + fan_out))
where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor. So it basically ensures that there is 
sufficient randomenss in our weights and therefore gets around the two issues that we just discussed. 


3.3
