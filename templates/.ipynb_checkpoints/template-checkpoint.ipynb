{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data & Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read data using pandas\n",
    "import pandas as pd\n",
    "pd.read_csv(filepath_or_buffer, sep=', ')\n",
    "df.to_csv('path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create and write to text\n",
    "file_object  = open('filename', 'mode') \n",
    "file = open('testfile.txt','w') \n",
    "file.write('Hello World') \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read data using urlib\n",
    "import urllib2\n",
    "homer = urllib2.urlopen('http://people.sc.fsu.edu/~jburkardt/datasets/sgb/homer.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create data\n",
    "S = pd.Series(np.random.randint(10,size=5),index=['A','B','C','D','E'])\n",
    "df_ = pd.DataFrame(index=index, columns=columns)\n",
    "df_ = df_.fillna(0) # with 0s rather than NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dropping\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df_country['gdp_missing'] = df_country['gdp'].isnull()\n",
    "\n",
    "##imputation\n",
    "df.ffill()\n",
    "df.bfill()\n",
    "df.col.fillna(df.col.mean())\n",
    "####apply transform to each group, fillna with group mean\n",
    "df_country.gdp = df_country.groupby('income_group').gdp.transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "##type transofmration\n",
    "df.price.astype(int)\n",
    "                \n",
    "##normalizing data\n",
    "###############DONT FORGeT TO NORMALIZE ALL YOUR FEATURES###############################\n",
    "df_country['population_density_zscore'] = stats.zscore(df_country['population_density'])\n",
    "\n",
    "##sort data\n",
    "df_country.sort_values('population_density_zscore',ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA & Data Manipulation & Slicing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary Views\n",
    "df.column.value_counts()\n",
    "df.info()\n",
    "df.col.describe()\n",
    "\n",
    "##Summary Metrics\n",
    "df.col.mean()\n",
    "df.col.median()\n",
    "df.col.std()\n",
    "\n",
    "##pandas aggregation\n",
    "##count number of nulls. In each group, count nulls\n",
    "df_country.groupby('income_group').gdp.apply(lambda x: sum(x.isnull()))\n",
    "\n",
    "##Index dataframe\n",
    "#index by label\n",
    "df.loc[['row0','row5],'col0']\n",
    "df.loc['row5',:] #5th row, all cols\n",
    "df.loc[:,['col1','col2']] ##all row, certain columns\n",
    "df.loc[(col1>50)&(col2<30),:]\n",
    "        \n",
    "#index by location\n",
    "df.iloc[0,0]\n",
    "df.iloc[0,:]# (first row, all columns)\n",
    "df.iloc[:2,:] #(first two rows, all columns)\n",
    "\n",
    "##Index Manipulation\n",
    "df.set_index('col')\n",
    "        \n",
    "###Join Data\n",
    "df1.join(df2, on='index',how='inner')\n",
    "pd.merge(df1,df2, left_on = 'col1',right_on='col2')\n",
    "\n",
    "##Pick only certain data\n",
    "data_indicators_to_keep = ['GDP (constant 2010 US$)','Population, total','Population density (people per sq. km of land area)','Unemployment, total (% of total labor force) (national estimate)']\n",
    "data_columns_to_keep = ['Country Code','Indicator Name','2016']\n",
    "df_data = df_data[df_data['Indicator Name'].isin(data_indicators_to_keep)][data_columns_to_keep]\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Visualization\n",
    "import matplotlib.pylplot as plt\n",
    "import seaborne as sns\n",
    "plt.hist(df.col)\n",
    "##vertical line to highlight data point\n",
    "plt.vlines(observed_mean_diff, ymin=0,ymax=.5, color='r')\n",
    "plt.xlabel('blah')\n",
    "plt.ylabel('yo')\n",
    "plt.title('hey')\n",
    "\n",
    "_ = plt.plot(fpr_dummy,tpr_dummy, color='blue', label='dummy') # curve for dummy\n",
    "_ = plt.step(fpr_logr,tpr_logr, color='red', label='logr') # curve for logr\n",
    "_ = plt.step(fpr_rf,tpr_rf, color='green', label='rf') # curve rf\n",
    "_ = plt.legend()# add a legend\n",
    "_ = plt.xlabel('False Positive Rate') # set x-axis label\n",
    "_ = plt.ylabel('True Postive Rate') # set y-axis label\n",
    "\n",
    "##pdf plot\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "sns.distplot(rand_mean_diffs)\n",
    "sns.boxplot(df_country.gdp_zscore,ax=ax[1])\n",
    "\n",
    "\n",
    "df,ax = plt.subplots()     \n",
    "\n",
    "\n",
    "##Axis range\n",
    "plt.xlim(-2,3)\n",
    "plt.ylim(-1,df.col.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = (observed_mean_diff - np.mean(rand_mean_diffs)) / np.std(rand_mean_diffs)\n",
    "##solving for t-test parameter, set one parameter to None\n",
    "tt_ind_solve_power(effect_size = observed_effect_size,  # what we just calculated\n",
    "                           nobs1 = n_A,         # the number of observations in A\n",
    "                           alpha = 0.05,        # our alpha level\n",
    "                           power = None,        # what we're interested in\n",
    "                           ratio = 1            # the ratio of number of observations of A and B\n",
    "                          )\n",
    "from scipy import stats\n",
    "stats.zscore(df_country['population_density'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##just pick some columns\n",
    "data_columns_to_keep = ['Country Code','Indicator Name','2016']\n",
    "df_data = df_data[df_data['Indicator Name'].isin(data_indicators_to_keep)][data_columns_to_keep]\n",
    "\n",
    "## F test for regression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "skb = SelectKBest(f_regression,k=3).fit(X_train_r,y_train_r)\n",
    "kept_columns = X_train_r.columns[skb.get_support()]\n",
    "\n",
    "##Using Feature Importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sfm = SelectFromModel(rf,threshold='mean',prefit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X,y_r,test_size=.2,random_state=42)\n",
    "\n",
    "##Regression Models\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_r = DummyRegressor(strategy='mean').fit(X_train_r,y_train_r)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train_r,y_train_r)\n",
    "\n",
    "rom sklearn.linear_model import ElasticNet\n",
    "en =  ElasticNet(l1_ratio=0.1).fit(X_train_r,y_train_r).predict(X_train_r)\n",
    "\n",
    "from sklearn import ensemble\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "##Classifiers\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_c = DummyClassifier(strategy='most_frequent').fit(X_train_c,y_train_c)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression().fit(X_train_c,y_train_c)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier(n_estimators=50).fit(X_train_c,y_train_c)\n",
    "\n",
    "##Parameter Tuning & GridSearchCV \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'l1_ratio':[.1,.5,.9,1]}\n",
    "gs =GridSearchCV(ElasticNet(),params).fit(X_train_r,y_train_r)\n",
    "params = {'n_estimators':[5,50,100]}\n",
    "gs = GridSearchCV(RandomForestClassifier(),params,cv=3).fit(X_train_c,y_train_c)\n",
    "\n",
    "\n",
    "print('gs best R2 score : {:.4f}'.format(gs.best_score_))\n",
    "print('gs best params: {}'.format(gs.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RMSE\n",
    "np.sqrt(mean_squared_error(y_test_r, model.predict(X_test_r)))\n",
    "\n",
    "##R2\n",
    "model_training_r2 = model.score(X_train_r,y_train_r)\n",
    "\n",
    "##cross-validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = cross_val_score(model,X_train_r,y_train_r,cv=5)\n",
    "cv.mean()\n",
    "\n",
    "##Residuals\n",
    "residuals = y_hat - y_train_r\n",
    "_ = plt.scatter(x=y_hat,y=residuals,alpha=.2)\n",
    "_ = plt.xlabel('y_hat')\n",
    "_ = plt.ylabel('residuals')\n",
    "\n",
    "##Precision-Recall, feed test y and predicted probabilities \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "pypos_rf = rf.predict_proba(X_test_c)[:,1]\n",
    "precision, recall, _= precision_recall_curve(y_test_c,pypos_rf)\n",
    "\n",
    "##ROC Curves\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_dummy,tpr_dummy,_ = roc_curve(y_test_c,pypos_dummy)\n",
    "fpr_logr,tpr_logr,_ = roc_curve(y_test_c,pypos_logr)\n",
    "fpr_rf,tpr_rf,_ = roc_curve(y_test_c,pypos_rf)\n",
    "_ = plt.plot(fpr_dummy,tpr_dummy, color='blue', label='dummy') # curve for dummy\n",
    "_ = plt.step(fpr_logr,tpr_logr, color='red', label='logr') # curve for logr\n",
    "_ = plt.step(fpr_rf,tpr_rf, color='green', label='rf') # curve rf\n",
    "_ = plt.legend()# add a legend\n",
    "_ = plt.xlabel('False Positive Rate') # set x-axis label\n",
    "_ = plt.ylabel('True Postive Rate') # set y-axis label\n",
    "\n",
    "##AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_test_c, rf.predict(X_test_c))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
